{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchcps.kernel.rkhs import GaussianKernel\n",
    "from torchcps.kernel.nn import (\n",
    "    KernelConv,\n",
    "    KernelMap,\n",
    "    KernelGraphFilter,\n",
    "    KernelPool,\n",
    "    KernelNorm,\n",
    "    Mixture,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datset of 2D signals with noise\n",
    "n_samples = 10_000\n",
    "n_sensors = 4\n",
    "n_measurements = 10\n",
    "n_dimensions = 2\n",
    "width = 1000\n",
    "\n",
    "noise_range = 10\n",
    "noise_bearing = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    targets = width / 2 * torch.rand(n_samples, n_dimensions) - width / 4\n",
    "    sensors = width / 2 * torch.rand(n_samples, n_sensors, n_dimensions) / 2\n",
    "\n",
    "    # relative position of targets to sensors\n",
    "    dx = targets[:, None] - sensors\n",
    "    dx = dx[..., None, :]\n",
    "    # add noise in polar coordinates\n",
    "    measurements_range = (\n",
    "        torch.norm(dx, dim=-1)\n",
    "        + torch.randn(n_samples, n_sensors, n_measurements) * noise_range\n",
    "    )\n",
    "    measurements_bearing = (\n",
    "        torch.atan2(dx[..., 1], dx[..., 0])\n",
    "        + torch.randn(n_samples, n_sensors, n_measurements) * noise_bearing\n",
    "    )\n",
    "    # convert back to cartesian coordinates\n",
    "    measurements = sensors[..., None, :] + torch.stack(\n",
    "        [\n",
    "            measurements_range * torch.cos(measurements_bearing),\n",
    "            measurements_range * torch.sin(measurements_bearing),\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "    # combine the n_sensors and n_measurements dimensions\n",
    "    measurements = measurements.reshape(\n",
    "        n_samples, n_sensors * n_measurements, n_dimensions\n",
    "    )\n",
    "    targets = targets[:, None, None, :]\n",
    "    measurements = measurements[:, None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate naive strategy of taking mean of measurements\n",
    "input_mean = measurements.mean(2, True)\n",
    "(input_mean - targets).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distribution of measurements and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, n_samples)\n",
    "    plt.plot(*targets[idx, 0, 0].numpy(), f\"C{i}x\", markersize=10)\n",
    "    plt.plot(*measurements[idx, 0].T.numpy(), f\"C{i}.\", markersize=5)\n",
    "    plt.xlim(-width / 2, width / 2)\n",
    "    plt.ylim(-width / 2, width / 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "n_channels = 8\n",
    "filter_kernels = 32\n",
    "n_weights = 16\n",
    "update_hidden = 32\n",
    "in_weights = 1\n",
    "out_weights = 1\n",
    "max_kernels = 1000\n",
    "fixed_positions = False\n",
    "sigma = [10.0, 10.0]\n",
    "filter_taps = 3\n",
    "normalize_graph = True\n",
    "penalty_weight = 1e-4\n",
    "\n",
    "\n",
    "class UpdatePositions(nn.Module):\n",
    "    def __init__(self, n_dimensions, n_weights, n_layers, n_hidden) -> None:\n",
    "        super().__init__()\n",
    "        dims = n_dimensions + n_weights\n",
    "        linears = []\n",
    "        for i in range(n_layers):\n",
    "            first_layer = i == 0\n",
    "            last_layer = i == n_layers - 1\n",
    "            linears.append(\n",
    "                nn.Linear(\n",
    "                    dims if first_layer else n_hidden,\n",
    "                    dims if last_layer else n_hidden,\n",
    "                )\n",
    "            )\n",
    "        self.linears = nn.ModuleList(linears)\n",
    "\n",
    "    def forward(self, input: Mixture):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Mixture): with the following elements.\n",
    "                positions: (batch_size, n_channels, in_kernels, n_dimensions)\n",
    "                weights: (batch_size, n_channels, in_kernels, in_weights)\n",
    "        \"\"\"\n",
    "        x = torch.cat([input.positions, input.weights], dim=-1)\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "            x = nn.LeakyReLU()(x)\n",
    "        positions = x[..., :n_dimensions] + input.positions\n",
    "        weights = x[..., n_dimensions:] + input.weights\n",
    "        return Mixture(positions, weights)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.n_layers = len(sigma)\n",
    "        self.nonlinearity = KernelMap(nn.LeakyReLU())\n",
    "        self.readin = KernelMap(nn.Linear(in_weights, n_weights))\n",
    "        self.readout = KernelMap(nn.Linear(n_weights, out_weights))\n",
    "        self.pool = KernelPool(\n",
    "            out_kernels=max_kernels,\n",
    "            kernel=GaussianKernel(),\n",
    "            strategy=\"largest\",\n",
    "            fit=False,\n",
    "        )\n",
    "\n",
    "        conv_layers = []\n",
    "        norm_layers = []\n",
    "        graph_layers = []\n",
    "        update_layers = []\n",
    "        for l in range(self.n_layers):\n",
    "            first_layer = l == 0\n",
    "            last_layer = l == self.n_layers - 1\n",
    "            norm_layers += [KernelNorm(1 if first_layer else n_channels, n_weights)]\n",
    "            update_layers += [UpdatePositions(2, n_weights, 2, update_hidden)]\n",
    "            conv_layers += [\n",
    "                KernelConv(\n",
    "                    filter_kernels=filter_kernels,\n",
    "                    in_channels=1 if first_layer else n_channels,\n",
    "                    out_channels=1 if last_layer else n_channels,\n",
    "                    n_dimensions=2,\n",
    "                    kernel_spread=3 * sigma[l] * filter_kernels**0.5,\n",
    "                    fixed_positions=fixed_positions,\n",
    "                    n_weights=n_weights,\n",
    "                )\n",
    "            ]\n",
    "            graph_layers += [\n",
    "                KernelGraphFilter(\n",
    "                    kernel=GaussianKernel(sigma[l]),\n",
    "                    in_weights=n_weights,\n",
    "                    out_weights=n_weights,\n",
    "                    filter_taps=filter_taps,\n",
    "                    normalize=normalize_graph,\n",
    "                )\n",
    "            ]\n",
    "        self.norm_layers = nn.ModuleList(norm_layers)\n",
    "        self.update_layers = nn.ModuleList(update_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.graph_layers = nn.ModuleList(graph_layers)\n",
    "\n",
    "    def forward(self, input: Mixture):\n",
    "        lost_energy = torch.zeros(self.n_layers, device=input.weights.device)\n",
    "\n",
    "        x = self.readin(input)\n",
    "        # x = self.nonlinearity(x)\n",
    "        for l in range(self.n_layers):\n",
    "            x = self.norm_layers[l](x)\n",
    "            # x = self.update_layers[l](x)\n",
    "            x = self.conv_layers[l](x)\n",
    "            x = self.graph_layers[l](x)\n",
    "            x = self.nonlinearity(x)\n",
    "            if l < self.n_layers - 1:\n",
    "                in_energy = x.weights.pow(2).mean(0).sum()\n",
    "                x = self.pool(x)\n",
    "                out_energy = x.weights.pow(2).mean(0).sum()\n",
    "                lost_energy[l] = 1 - out_energy / in_energy\n",
    "        x: Mixture = self.readout(x)\n",
    "        x = self.nonlinearity(x)\n",
    "        return x, lost_energy.mean()\n",
    "\n",
    "\n",
    "dataset = TensorDataset(measurements.cuda(), targets.cuda())\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = Model().cuda()\n",
    "\n",
    "\n",
    "# positions should not be regularized\n",
    "parameters = list(model.named_parameters())\n",
    "positions = [p for n, p in parameters if \"_positions\" in n]\n",
    "weights = [p for n, p in parameters if \"_positions\" not in n]\n",
    "optimizer = AdamW(\n",
    "    [dict(params=weights), dict(params=positions, lr=1e-2, weight_decay=0.0)],\n",
    "    lr=1e-2,\n",
    "    weight_decay=0,\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    \"min\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    cooldown=2,\n",
    ")\n",
    "\n",
    "\n",
    "mse_values = []\n",
    "for i in range(20):\n",
    "    pbar = tqdm(dataloader, total=len(dataloader))\n",
    "    total_loss = 0\n",
    "    for x, y in pbar:\n",
    "        x_weights = torch.ones(*x.shape[:-1], device=\"cuda\")[..., None]\n",
    "        y_weights = torch.ones(*y.shape[:-1], device=\"cuda\")[..., None]\n",
    "        (z, z_weights), lost_energy = model.forward(Mixture(x, x_weights))\n",
    "\n",
    "        kernel = GaussianKernel(sigma[-1])\n",
    "        mse = kernel.squared_error(y, y_weights, z, z_weights).mean(0).sum()\n",
    "        mse_values.append(mse.item())\n",
    "        loss = mse + penalty_weight * lost_energy\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            loss=loss.item(),\n",
    "            mse=mse.item(),\n",
    "            lost_energy=lost_energy.item(),\n",
    "            lr=optimizer.param_groups[0][\"lr\"],\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mse_values[-10:]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mse_values, label=\"MSE\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_rkhs(X: Mixture, sigma: float, width: float, resolution: int, relu=False):\n",
    "    XY = (\n",
    "        torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.linspace(-width / 2, width / 2, resolution),\n",
    "                torch.linspace(-width / 2, width / 2, resolution),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        .reshape(-1, 2)\n",
    "        .to(X.positions.device)\n",
    "    )\n",
    "    kernel = GaussianKernel(sigma)\n",
    "    values = kernel(XY, X.positions[0, 0]) @ X.weights\n",
    "    XY = XY.reshape(resolution, resolution, 2).detach()\n",
    "    values = values.reshape(resolution, resolution).detach()\n",
    "    return values, XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "indices = np.random.choice(n_samples, n_test, replace=False)\n",
    "\n",
    "model.eval()\n",
    "mae_mean = 0\n",
    "mae_mode = 0\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(indices):\n",
    "        x = measurements[None, idx, ...].cuda()\n",
    "        y = targets[None, idx, None, :].cuda()\n",
    "        x_weights = torch.ones(*x.shape[:-1], device=x.device)[..., None]\n",
    "        y_weights = torch.ones(*y.shape[:-1], device=y.device)[..., None]\n",
    "\n",
    "        Z, _ = model(Mixture(x, x_weights))\n",
    "\n",
    "        # output argmax\n",
    "        values, XY = raster_rkhs(Z, sigma[-1], width, 1000)\n",
    "        # values.relu_()\n",
    "        # expectation\n",
    "        mean_xy = (XY * values[..., None]).sum((0, 1)) / values.sum()\n",
    "        mae_mean += ((mean_xy - y.squeeze()).abs()).sum() / n_test\n",
    "        # mode\n",
    "        mode_xy = XY.reshape(-1, 2)[torch.argmax(values)]\n",
    "        mae_mode += ((mode_xy - y.squeeze()).abs()).sum() / n_test\n",
    "print(f\"Mean Absolute Error (MEAN): {mae_mean.item():.2f}\")\n",
    "print(f\"Mean Absolute Error (MODE): {mae_mode.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, n_samples)\n",
    "resolution = 1000\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = measurements[None, idx, ...].cuda()\n",
    "    y = targets[None, idx, None, :].cuda()\n",
    "    x_weights = torch.ones(*x.shape[:-1], device=x.device)[..., None]\n",
    "    y_weights = torch.ones(*y.shape[:-1], device=y.device)[..., None]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, z_weights = model(Mixture(x, x_weights))[0]\n",
    "    model.train()\n",
    "\n",
    "    # squeeze all the tensors\n",
    "    x = x.squeeze().cpu()\n",
    "    y = y.squeeze().cpu()\n",
    "\n",
    "    extent = [-width / 2, width / 2, -width / 2, width / 2]\n",
    "    values, XY = raster_rkhs(Mixture(z, z_weights), sigma[-1], width, 1000)\n",
    "    values.relu_()\n",
    "    # naive way to make predictions\n",
    "    input_mean = x.mean(0)\n",
    "    # expectation\n",
    "    mean_xy = (XY * values[..., None]).sum((0, 1)) / values.sum()\n",
    "    # mode\n",
    "    mode_xy = XY.reshape(-1, 2)[torch.argmax(values)]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(values.T.cpu().detach(), extent=extent, origin=\"lower\")\n",
    "\n",
    "plt.plot(*x.T, \".\", label=\"Measurements\")\n",
    "plt.plot(*y, \"x\", label=\"Target\")\n",
    "plt.plot(*input_mean, \"o\", label=\"Mean of Measurements\")\n",
    "plt.plot(*mean_xy.detach().cpu(), \"o\", label=\"Mean of CNN output\")\n",
    "plt.plot(*mode_xy.detach().cpu(), \"o\", label=\"Mode of CNN output\")\n",
    "\n",
    "# plt.xlim(extent[0], extent[1])\n",
    "# plt.ylim(extent[2], extent[3])\n",
    "# axis limits y +- 100\n",
    "plt.xlim(y[0] - 100, y[0] + 100)\n",
    "plt.ylim(y[1] - 100, y[1] + 100)\n",
    "\n",
    "plt.legend()\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_filter(\n",
    "    X: Mixture,\n",
    "    sigma: float,\n",
    "    weight_idx: int,\n",
    "    channel: tuple[int, int],\n",
    "    width: float,\n",
    "    resolution: int,\n",
    "):\n",
    "    X_ = Mixture(\n",
    "        X.positions[channel[0], channel[1], None, None, ...],\n",
    "        X.weights[channel[0], channel[1], None, None, :, weight_idx, None].contiguous(),\n",
    "    )\n",
    "    return raster_rkhs(X_, sigma, width, resolution)[0].detach().cpu()\n",
    "\n",
    "\n",
    "# Plot the CNN Filter at the lth layer\n",
    "l = 0\n",
    "channel = (0, 0)\n",
    "\n",
    "\n",
    "conv_layers = model.conv_layers\n",
    "positions = conv_layers[l].kernel_positions\n",
    "weights = conv_layers[l].kernel_weights\n",
    "assert isinstance(positions, torch.Tensor) and isinstance(weights, torch.Tensor)\n",
    "\n",
    "n_weights = weights.shape[3]\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    n_weights,\n",
    "    figsize=(n_weights, 1),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "for i in range(n_weights):\n",
    "    rkhs = Mixture(positions, weights)\n",
    "    filter_width = sigma[l] * 10\n",
    "    extent = np.array([-1, 1, -1, 1]) * filter_width / 2\n",
    "\n",
    "    if n_weights > 1:\n",
    "        ax = axs[i]\n",
    "    else:\n",
    "        ax = axs\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax.imshow(\n",
    "        raster_filter(rkhs, sigma[l], i, channel, width=filter_width, resolution=32),\n",
    "        extent=extent,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
